## Navigate to the directory webapp-color under the /root directory.


```ruby
controlplane ~ âžœ  ls
webapp-color

controlplane ~ âžœ  cd webapp-color/

controlplane ~/webapp-color is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  ls
Chart.yaml  templates  values.yaml

controlplane ~/webapp-color is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  cat Chart.yaml 
apiVersion: v2
name: webapp-color
description: A Helm chart for Webapp Color Application

type: application

# This is the chart version. This version number should be incremented each time you make changes
# to the chart and its templates, including the app version.
# Versions are expected to follow Semantic Versioning (https://semver.org/)
version: 0.1.0

# This is the version number of the application being deployed. This version number should be
# incremented each time you make changes to the application. Versions are not expected to
# follow Semantic Versioning. They should reflect the version the application is using.
# It is recommended to use it with quotes.
appVersion: "v1"

controlplane ~/webapp-color is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  cat values.yaml 
# Default values for webapp-color.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: kodekloud/webapp-color
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []

name: webapp-color

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: webapp-sa
  labels:
    - frontend
    - web
    - proxy

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: NodePort
  port: 8080
  nodePort: 30080

environment: development


configMap:
  name: webapp-color-configmap

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
nodeSelector: {}

tolerations: []

affinity: {}

controlplane ~/webapp-color is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  cd templates/

controlplane ~/webapp-color/templates âžœ  ls
configmap.yaml  deployment.yaml  helpers.tpl  serviceaccount.yaml  service.yaml

controlplane ~/webapp-color/templates âžœ  cat configmap.yaml 
apiVersion: v1
metadata:
  name: {{ .Values.configMap.name }}
  namespace: default
kind: ConfigMap
data:
  {{- if eq .Values.environment "production" }}
    APP_COLOR: pink
  {{- else if eq .Values.environment "development" }}
    APP_COLOR: darkblue
  {{- else }}
    APP_COLOR: green
  {{- end }}

controlplane ~/webapp-color/templates âžœ  cat deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webapp-color
  name: {{ .Values.name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp-color
  template:
    metadata:
      labels:
        app: webapp-color
    spec:
      containers:
      - image: kodekloud/webapp-color
        name: webapp-color
        envFrom:
         - configMapRef:
                name: {{ .Values.configMap.name }}

controlplane ~/webapp-color/templates âžœ  cat helpers.tpl 
{{/*
Common labels
*/}}
{{- define "labels" }}
  unit: dev
{{- end }}

controlplane ~/webapp-color/templates âžœ  cat serviceaccount.yaml 
{{- with .Values.serviceAccount.create }}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ $.Values.serviceAccount.name }}
```

## You will be able to see a .tpl file inside the templates directory of the webapp-color chart.


There is an issue with the file name. Fix it.

The template file should start with an underscore. Else it will be considered a Kubernetes manifest file.


![image](https://github.com/Althaf-official/Helm/assets/105126131/0210c493-09eb-4863-81aa-0c19f024bfb1)

## Now add the label added in the template file to the deployment.yaml file.


Use include statement and correct indentation.


```ruby
The solution deployment files should be as follows:

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
  {{- include "labels" . | indent 2 }}
    app: webapp-color
  name: {{ .Values.name }}
spec:
  replicas: 1
  selector:
    matchLabels:
    {{- include "labels" . | indent 4 }}
      app: webapp-color
  template:
    metadata:
      labels:
      {{- include "labels" . | indent 6 }}
        app: webapp-color
    spec:
      containers:
      - image: kodekloud/webapp-color
        name: webapp-color
        envFrom:
         - configMapRef:
                name: {{ .Values.configMap.name }}



==================================================================================================================================================================================


controlplane ~/webapp-color/templates âžœ  cat deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webapp-color
  name: {{ .Values.name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp-color
  template:
    metadata:
      labels:
        app: webapp-color
    spec:
      containers:
      - image: kodekloud/webapp-color
        name: webapp-color
        envFrom:
         - configMapRef:
                name: {{ .Values.configMap.name }}

controlplane ~/webapp-color/templates âžœ  vi deployment.yaml 

controlplane ~/webapp-color/templates âžœ  cat deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
  {{- include "labels" . | indent 2 }}
    app: webapp-color
  name: {{ .Values.name }}
spec:
  replicas: 1
  selector:
    matchLabels:
    {{- include "labels" . | indent 4 }}
      app: webapp-color
  template:
    metadata:
      labels:
      {{- include "labels" . | indent 6 }}
        app: webapp-color
    spec:
      containers:
      - image: kodekloud/webapp-color
        name: webapp-color
        envFrom:
         - configMapRef:
                name: {{ .Values.configMap.name }}
```


## Now define a selectorLabels section in the template file. You should use the selector unit: dev for the service.


Note: We have already updated the selector field in the service.yaml file.

Check the service.yaml file and add the webapp-color.selectorLabels in the template file.

The _helpers.tpl file should be as follows:
```ruby
{{/*
Common labels
*/}}
{{- define "labels" }}
  unit: dev
{{- end }}


{{- define "webapp-color.selectorLabels" }}
  unit: dev
{{- end }}

```


![image](https://github.com/Althaf-official/Helm/assets/105126131/e3ac5c0c-a20b-4edc-abb7-ee0d7e586f42)


## Now navigate to the directory /root/webapp where we have an Apache helm chart repository.

```ruby
controlplane ~ âžœ  ls
webapp  webapp-color

controlplane ~ âžœ  cd webapp

controlplane ~/webapp is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  ls
Chart.yaml  templates  values.yaml

controlplane ~/webapp is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  cat Chart.yaml 
apiVersion: v2
appVersion: 2.4.48
description: An Apache Application
name: webapp
type: application
version: 0.1.0

controlplane ~/webapp is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  cat values.yaml 
# Default values for webapp.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: httpd
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

nameOverride: webapp
fullnameOverride: my-webapp

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: webapp

service:
  type: NodePort
  port: 80
  nodePort: 31080

controlplane ~/webapp is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  cd templates/

controlplane ~/webapp/templates âžœ  ls
deployment.yaml  _helpers.tpl  install-job.yaml  NOTES.txt  serviceaccount.yaml  service.yaml

controlplane ~/webapp/templates âžœ  cat deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "webapp.fullname" . }}
  labels:
    {{- include "webapp.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "webapp.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "webapp.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "webapp.serviceAccountName" . }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http

controlplane ~/webapp/templates âžœ  cat service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: {{ include "webapp.fullname" . }}
  labels:
    {{- include "webapp.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
      nodePort: {{ .Values.service.nodePort }}
  selector:
    {{- include "webapp.selectorLabels" . | nindent 4 }}

controlplane ~/webapp/templates âžœ  cat serviceaccount.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ .Values.serviceAccount.name }}
  labels:
    {{- include "webapp.labels" . | nindent 4 }}

controlplane ~/webapp/templates âžœ  cat _helpers.tpl 
{{/*
Expand the name of the chart.
*/}}
{{- define "webapp.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "webapp.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "webapp.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "webapp.labels" -}}
helm.sh/chart: {{ include "webapp.chart" . }}
{{ include "webapp.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "webapp.selectorLabels" -}}
app.kubernetes.io/name: {{ include "webapp.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "webapp.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "webapp.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}

controlplane ~/webapp/templates âžœ  ls
deployment.yaml  _helpers.tpl  install-job.yaml  NOTES.txt  serviceaccount.yaml  service.yaml

controlplane ~/webapp/templates âžœ  cat install-job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  name: install-hook
spec:
  template:
    spec:
      containers:
      - name: install-hook
        image: alpine
        command: ["echo",  "Successful Installation"]
      restartPolicy: Never
  backoffLimit: 4

controlplane ~/webapp/templates âžœ  cat NOTES.txt 
1. Get the application URL by running these commands:
{{- if contains "NodePort" .Values.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "webapp.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "webapp.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "webapp.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo http://$SERVICE_IP:{{ .Values.service.port }}
{{- else if contains "ClusterIP" .Values.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "webapp.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
{{- end }}

controlplane ~/webapp/templates âžœ  
```

## Now modify the install-job.yaml so that it will only execute after all resources are loaded into Kubernetes.

Use post-install hook in the install-job.yaml.

```ruby
The install-job.yaml should be modified as below:

apiVersion: batch/v1
kind: Job
metadata:
  name: install-hook
  annotations:
     "helm.sh/hook": post-install
spec:
  template:
    spec:
      containers:
      - name: install-hook
        image: alpine
        command: ["echo",  "Successful Installation"]
      restartPolicy: Never
  backoffLimit: 4
```


![image](https://github.com/Althaf-official/Helm/assets/105126131/a516a05c-d6aa-48e4-ba05-f1c8931135e5)


![image](https://github.com/Althaf-official/Helm/assets/105126131/98df3cc5-bfe2-41d3-8522-b6dc59174f9f)

## Which hook deletion policy deletes the previous resource before a new hook is launched?

before-hook-creation policy deletes the previous resource before a new hook is launched.

## Install the webapp chart repository.


The release name should be httpd.

After creation, you can see the pod status Completed, which is created by a job called install-hook. Check the pod log to view the output.

```ruby
controlplane ~/webapp/templates âžœ  cd ~

controlplane ~ âžœ  ls
webapp  webapp-color

controlplane ~ âžœ  helm install httpd webapp
NAME: httpd
LAST DEPLOYED: Tue Nov 21 06:45:57 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services my-webapp)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

controlplane ~ âžœ  kubectl get all
NAME                            READY   STATUS      RESTARTS   AGE
pod/install-hook-wpxf2          0/1     Completed   0          48s
pod/my-webapp-9f568f885-8zd45   1/1     Running     0          48s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        68m
service/my-webapp    NodePort    10.107.202.113   <none>        80:31080/TCP   49s

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-webapp   1/1     1            1           49s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/my-webapp-9f568f885   1         1         1       49s

NAME                     COMPLETIONS   DURATION   AGE
job.batch/install-hook   1/1           5s         48s

controlplane ~ âžœ  kubectl describe pod install-hook-wpxf2
Name:             install-hook-wpxf2
Namespace:        default
Priority:         0
Service Account:  default
Node:             controlplane/192.21.11.3
Start Time:       Tue, 21 Nov 2023 06:45:58 -0500
Labels:           batch.kubernetes.io/controller-uid=661a5cbb-23c7-4216-be2a-8c861c13a561
                  batch.kubernetes.io/job-name=install-hook
                  controller-uid=661a5cbb-23c7-4216-be2a-8c861c13a561
                  job-name=install-hook
Annotations:      <none>
Status:           Succeeded
IP:               10.244.0.5
IPs:
  IP:           10.244.0.5
Controlled By:  Job/install-hook
Containers:
  install-hook:
    Container ID:  containerd://c4c75ee8f3105dcc7f0694ed2c7aaf008bdc50925b7de840016c0b1fa6e7ec67
    Image:         alpine
    Image ID:      docker.io/library/alpine@sha256:eece025e432126ce23f223450a0326fbebde39cdf496a85d8c016293fc851978
    Port:          <none>
    Host Port:     <none>
    Command:
      echo
      Successful Installation
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 21 Nov 2023 06:46:00 -0500
      Finished:     Tue, 21 Nov 2023 06:46:00 -0500
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bd2kw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-bd2kw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  2m9s  default-scheduler  Successfully assigned default/install-hook-wpxf2 to controlplane
  Normal  Pulling    2m8s  kubelet            Pulling image "alpine"
  Normal  Pulled     2m7s  kubelet            Successfully pulled image "alpine" in 1.228430395s (1.228494293s including waiting)
  Normal  Created    2m7s  kubelet            Created container install-hook
  Normal  Started    2m7s  kubelet            Started container install-hook

controlplane ~ âžœ  
```

## Now create a new job named upgrade-hook with annotation hook as post-upgrade and upgrade the chart which you installed previously.


App version should be 2.4.51 and Chart version should be 0.2.

Image to be used in the job should be alpine.

The job should provide an output Successful Upgradation.


Create a new job named upgrade-hook and run the command: helm upgrade.

```ruby
First update the Chart.yaml as follows:

apiVersion: v2
appVersion: 2.4.51
description: An Apache Application
name: webapp
type: application
version: 0.2
Create a new file upgrade-job.yaml as shown below:

apiVersion: batch/v1
kind: Job
metadata:
  name: upgrade-hook
  annotations:
     "helm.sh/hook": post-upgrade
spec:
  template:
    spec:
      containers:
      - name: upgrade-hook
        image: alpine
        command: ["echo",  "Successful Upgradation"]
      restartPolicy: Never
  backoffLimit: 4
Then run the command helm upgrade httpd webapp.

========================================================================================================================================================================

controlplane ~ âžœ  cd webapp

controlplane ~/webapp is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  ls
Chart.yaml  templates  values.yaml

controlplane ~/webapp is ðŸ“¦ v0.1.0 via âŽˆ v3.13.1 âžœ  vi Chart.yaml 

controlplane ~/webapp via âŽˆ v3.13.1 âžœ  cd templates/

controlplane ~/webapp/templates âžœ  ls
deployment.yaml  _helpers.tpl  install-job.yaml  NOTES.txt  serviceaccount.yaml  service.yaml

controlplane ~/webapp/templates âžœ  cat > upgrade-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: upgrade-hook
  annotations:
     "helm.sh/hook": post-upgrade
spec:
  template:
    spec:
      containers:
      - name: upgrade-hook
        image: alpine
        command: ["echo",  "Successful Upgradation"]
      restartPolicy: Never
  backoffLimit: 4
controlplane ~/webapp/templates âžœ  cd ~

controlplane ~ âžœ  helm upgrade httpd webapp
Release "httpd" has been upgraded. Happy Helming!
NAME: httpd
LAST DEPLOYED: Tue Nov 21 06:54:05 2023
NAMESPACE: default
STATUS: deployed
REVISION: 3
TEST SUITE: None
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services my-webapp)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

controlplane ~ âžœ  
```
